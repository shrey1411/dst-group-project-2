{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This project explored how neural network parameters, such as batch size and learning rate, impact the performance using brain tumour data. In our findings, we found that there was an interplay between the parameters and the architecture. Indeed, we had to make adjustments like altering pooling mechanisms or depth in order to fit some models, and to better understand the effect of some parameters. The significance of this work is evident in domains like medicine, where neural networks are used for life-saving tasks such as disease diagnosis. \n",
    "\n",
    "Parallelism played a crucial role in our project. The use of GPUs drastically reduced training times—100 epochs on a GPU took 10–15 minutes, compared to 40 minutes for 15 epochs on a CPU. This dramatic difference underscores the necessity of leveraging parallel computing for complex neural network tasks.\n",
    "\n",
    "Scalability was a key consideration of the project. In our investigations, we increased the dataset size by 25% and image resolution by 16x (from 64 x 64 to 256 x 256). This highlighted the trade-off between runtime and accuracy, as models took longer to run, but did not always give better performance. \n",
    "\n",
    "However, the project faced several limitations. Restricted GPU runtime on platforms like Google Colab limited the scope of our experiments. Also, the use of \"semi-transfer learning\" in some investigations constrained our ability to experiment with deeper and more diverse architectures. Finally, relying on pre-processed Kaggle data left open questions about how our findings would generalise to raw, noisy datasets where we have non-uniform MRI scans and machines. \n",
    "\n",
    "In conclusion, while this project offered valuable insights, addressing its limitations is crucial to ensuring neural networks are robust, scalable, and ready for high-stakes applications like healthcare."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
