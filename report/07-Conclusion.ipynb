{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This project explored how neural network parameters, such as batch size and learning rate, impact the performance using brain tumour data. In our findings, we found that there was an interplay between the parameters and the architecture. Indeed, we had to make adjustments like altering pooling mechanisms or depth in order to fit some models, and to better understand the effect of some parameters. The significance of this work is evident in domains like medicine, where neural networks are used for life-saving tasks such as disease diagnosis. \n",
    "\n",
    "Parallelism played a crucial role in our project. The use of GPUs drastically reduced training times—100 epochs on a GPU took 10–15 minutes, compared to 40 minutes for 15 epochs on a CPU. This dramatic difference underscores the necessity of leveraging parallel computing for complex neural network tasks.\n",
    "\n",
    "Scalability was a key consideration of the project. In our investigations, we increased the dataset size by 25% and image resolution by 16x (from 64 x 64 to 256 x 256). This highlighted the trade-off between runtime and accuracy, as models took longer to run, but did not always give better performance. \n",
    "\n",
    "However, the project faced several limitations. Restricted GPU runtime on platforms like Google Colab limited the scope of our experiments. Also, the use of \"semi-transfer learning\" in some investigations constrained our ability to experiment with deeper and more diverse architectures. Finally, relying on pre-processed Kaggle data left open questions about how our findings would generalise to raw, noisy datasets where we have non-uniform MRI scans and machines. \n",
    "\n",
    "In conclusion, while this project offered valuable insights, addressing its limitations is crucial to ensuring neural networks are robust, scalable, and ready for high-stakes applications like healthcare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion [Version after our meeting so you can check]\n",
    "\n",
    "In this project, we have investigated how the parameters of a neural network affect its performance. We fine-tuned several parameters, including but not limited to batch size, number of epochs and learning rate. Touch on the importance of the application domain - e.g life-saving, importance in medicine. \n",
    "\n",
    "We found that the activation function was not the only thing important. We needed to also vary the architecture to capture the complexities in the dataset. \n",
    "\n",
    "Scalability aspect: how the training was affected by increase in volume of data. Managing tradeoff between runtime and performance. \n",
    "\n",
    "Parallelism was essential in allowing us to implement anything. Not using GPUs would mean having a computation running for hours on personal computers. Took 10-15 minutes for the models to fit with 100 epochs with a GPU for a group member. Using a CPU only for 15 epochs took around 40 minutes. So we got great gains in performance. \n",
    "\n",
    "GPU Colab runtime limits was a limitation of the project and prevented some of us from doing investigations that were as deep as we would have liked. Base model for the Swish model was semi-transfer learning so there was a limitation on the depth/width/architecture. \n",
    "\n",
    "Further research needs to be done to generalise the findings to the real-world. Noise in real-world, MRI scans can be non-uniform, e.g. machine not uniform, etc. Pre-processing stage. Our data came pre-processed. How well can we do this for real data?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
